<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Shao  ZHANG</title>
    <meta name="author" content="Shao  ZHANG">
    <meta name="description" content="">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B0&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://shaozhang.info/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
                        <!--<a href="mailto:%73%68%61%6F%7A%68%61%6E%67@%73%6A%74%75.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
             --><a href="https://orcid.org/0000-0002-0111-0776" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=UG36L2YAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.semanticscholar.org/author/2116577679" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a>
            <a href="https://github.com/ShaoZhang0115#%20your%20GitHub%20user%20name" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">Bio<span class="sr-only">(current)</span></a>
              </li>
              

              <!--  -->
              <!-- CV -->
              <!-- <li class="nav-item "> -->
                <!-- <a class="nav-link" href="/assets/pdf/CV_Shao_Zhang.pdf" target="_blank" rel="noopener noreferrer">cv -->
                  <!---->
                <!-- </a> -->
              <!-- </li> -->
              <!-- -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
</li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv">cv</a>
</li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
</li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Shao</span>  ZHANG
          </h1>
          <p class="desc">Ph.D. Candidate, <a href="https://www.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank"> Shanghai Jiao Tong University</a>, Shanghai, China.</p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/photo-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/photo-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/photo-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/photo.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="photo.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<!--
            <div class="address">
              <p>Shanghai Jiao Tong University</p> <p>Shanghai, China</p>

            </div> -->
            
            <div class="email">
              Email: <a href="mailto:shaozhang@sjtu.edu.cn">shaozhang@sjtu.edu.cn</a>
              <!-- <a href="mailto:zhang.shao.1@northeastern.edu">zhang.shao.1@northeastern.edu</a> -->
            </div>
            
          </div>

          <div class="clearfix">
            <p>Shao ZHANG is currently a Ph.D. candidate at Shanghai Jiao Tong University, supervised by <a href="https://yingwen.io/" rel="external nofollow noopener" target="_blank">Prof. Ying WEN</a> and <a href="https://www.cs.sjtu.edu.cn/~wang-xb/" rel="external nofollow noopener" target="_blank">Prof. Xinbing WANG</a>.</p>

<p>From 2023 to 2024, Shao was a visiting student at Northeastern University, Boston Campus.
Shao obtained her MSc in Multimedia and Entertainment Technology (Game Development stream) from <a href="https://www.sd.polyu.edu.hk/en/" rel="external nofollow noopener" target="_blank">School of Design, The Hong Kong Polytechnic University</a> in 2020. Before that, Shao earned her B.Eng. in Industrial Design from <a href="http://design.hnu.edu.cn/" rel="external nofollow noopener" target="_blank">School of Design, Hunan University</a> in 2019. During her undergraduate study, she was a member of <a href="http://xiaolab.net/" rel="external nofollow noopener" target="_blank">XiaoLab</a> and was supervised by <a href="http://xiaolab.net/" rel="external nofollow noopener" target="_blank">Prof. Sheng XIAO</a> and <a href="http://ylsy.hnu.edu.cn/info/1353/9629.htm" rel="external nofollow noopener" target="_blank">Prof. Beibei ZHAN</a>.</p>

<p>Shao‚Äôs research interests include <strong>Human-AI Collaboration</strong> and <strong>Multi-agent System</strong>. 
Specifically, she is now focusing on:</p>
<ul>
  <li>
<strong>AI-in-the-Loop</strong> deployment for human-AI collaboration systems in various real-world scenarios,</li>
  <li>
<strong>Zero-shot Coordination</strong> (ZSC) in human-AI collaboration via multi-agent reinforcement learning (MARL),</li>
  <li>
<strong>Language Agent</strong> in Real-time Simultaneous human-AI collaboration via Cognitive Theory like Theory of Mind (ToM) and Dual Process Theory (DPT).</li>
</ul>

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>          
          <div class="news">
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
               
                <tr>
                    <th scope="row">Dec 29, 2024</th>
                    <td>
                      Our paper ‚Äú<a href="">PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning</a>‚Äù gets accepted by AAMAS 2025.
 
                    </td>
                  </tr> 
                <tr>
                    <th scope="row">Sep 26, 2024</th>
                    <td>
                      Two papers are accepted by NeurIPS 2024 (1 Main Track and 1 D&amp;B Track)! See you in Vancouver!

<ul>
  <li>
<a href="https://arxiv.org/abs/2310.05208" rel="external nofollow noopener" target="_blank">ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination</a> (D&amp;B Track)</li>
  <li>
<a href="https://arxiv.org/abs/2402.12416" rel="external nofollow noopener" target="_blank">Aligning Individual and Collective Objectives in Multi-Agent Cooperation</a> (Main Track)</li>
</ul>
 
                    </td>
                  </tr> 
                <tr>
                    <th scope="row">May 22, 2024</th>
                    <td>
                      Our paper ‚Äú<a href="https://arxiv.org/abs/2306.03034" rel="external nofollow noopener" target="_blank">Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination</a>‚Äù gets accepted by Journal of Artificial Intelligence Research (JAIR).
 
                    </td>
                  </tr> 
                <tr>
                    <th scope="row">Jan 19, 2024</th>
                    <td>
                      Our Paper <a href="https://arxiv.org/abs/2309.12368" rel="external nofollow noopener" target="_blank">Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis</a> is accepted by CHI‚Äô24! See you in Hawaii!üèñÔ∏è
 
                    </td>
                  </tr> 
                 
                 
                <tr>
                    <th scope="row">Aug 15, 2023</th>
                    <td>
                      A white paper about large-scale pre-trained and multi-agent reinforcement learning published in CCCF: <a href="https://dl.ccf.org.cn/article/articleDetail.html?type=xhtx_thesis&amp;_ack=1&amp;id=6605918436214784#10006-weixin-1-52626-6b3bffd01fdde4900130bc5a2751b6d1" rel="external nofollow noopener" target="_blank">Multi-agent Reinforcement Learning in (Open-ended) Multimodal Environments (in Chinese)</a>.

 
                    </td>
                  </tr> 
                <tr>
                    <th scope="row">Apr 25, 2023</th>
                    <td>
                      Our Paper <a href="https://openreview.net/forum?id=iMVMxYab0z" rel="external nofollow noopener" target="_blank">Cooperative Open-ended Learning Framework for Zero-Shot Coordination</a> is accepted in ICML 2023! Can‚Äôt wait to see you in Hawaii on July! üèñÔ∏è
 
                    </td>
                  </tr> 
                 
                 
                 
                 
              </table>
            </div> 
          </div>


          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
<div class="publications">
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Language Agent</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang2025dpt" class="col-sm-8">
        <!-- Title -->
        <div class="title">Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration</div>
        <!-- Author -->
        <div class="author">
        

        <em>Shao Zhang*</em>,¬†<a href="https://xihuai18.github.io/" rel="external nofollow noopener" target="_blank">Xihuai Wang*</a>,¬†Wenhao Zhang,¬†Chaoran Li,¬†Junru Song,¬†Tingyu Li,¬†Lin Qiu,¬†Xuezhi Cao,¬†Xunliang Cai,¬†Wen Yao,¬†<a href="https://wnzhang.net/" rel="external nofollow noopener" target="_blank">Weinan Zhang</a>,¬†Xinbing Wang,¬†and¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Preprint Under Review</em>, 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/abs/2502.11882.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/sjtu-marl/DPT-Agent" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent‚Äôs System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent‚Äôs System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2025dpt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang*, Shao and Wang*, Xihuai and Zhang, Wenhao and Li, Chaoran and Song, Junru and Li, Tingyu and Qiu, Lin and Cao, Xuezhi and Cai, Xunliang and Yao, Wen and Zhang, Weinan and Wang, Xinbing and Wen, Ying}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Language Agent</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang2024mutualtheorymindhumanai" class="col-sm-8">
        <!-- Title -->
        <div class="title">Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task</div>
        <!-- Author -->
        <div class="author">
        

        <em>Shao Zhang*</em>,¬†<a href="https://xihuai18.github.io/" rel="external nofollow noopener" target="_blank">Xihuai Wang*</a>,¬†Wenhao Zhang,¬†Yongshan Chen,¬†Landi Gao,¬†Dakuo Wang,¬†<a href="https://wnzhang.net/" rel="external nofollow noopener" target="_blank">Weinan Zhang</a>,¬†Xinbing Wang,¬†and¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Preprint Under Review</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/abs/2409.08811.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Theory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team‚Äôs performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent‚Äôs ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results‚Äô implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2024mutualtheorymindhumanai</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang*, Shao and Wang*, Xihuai and Zhang, Wenhao and Chen, Yongshan and Gao, Landi and Wang, Dakuo and Zhang, Weinan and Wang, Xinbing and Wen, Ying}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2409.08811}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.HC}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint Under Review}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Zero-shot Coordination</abbr></div>

        <!-- Entry bib key -->
        <div id="wang2023quantifying" class="col-sm-8">
        <!-- Title -->
        <div class="title">ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://xihuai18.github.io/" rel="external nofollow noopener" target="_blank">Xihuai Wang*</a>,¬†<em>Shao Zhang*</em>,¬†Wenhao Zhang,¬†Wentao Dong,¬†Jingxiao Chen,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†and¬†<a href="https://wnzhang.net/" rel="external nofollow noopener" target="_blank">Weinan Zhang</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>The 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2310.05208.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/sjtu-marl/ZSC-Eval" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Zero-shot coordination (ZSC) is a new challenge focusing on generalizing learned coordination skills to unseen partners. Existing methods train the ego agent with partners from pre-trained or evolving populations. The agent‚Äôs ZSC capability is typically evaluated with a few evaluation partners, including human and agent, and reported by mean returns. Current evaluation methods for ZSC capability still need to improve in constructing diverse evaluation partners and comprehensively measuring the ZSC capability. We aim to create a reliable, comprehensive, and efficient evaluation method for ZSC capability. We formally define the ideal ‚Äôdiversity-complete‚Äô evaluation partners and propose the best response (BR) diversity, which is the population diversity of the BRs to the partners, to approximate the ideal evaluation partners. We propose an evaluation workflow including ‚Äôdiversity-complete‚Äô evaluation partners construction and a multi-dimensional metric, the Best Response Proximity (BR-Prox) metric. BR-Prox quantifies the ZSC capability as the performance similarity to each evaluation partner‚Äôs approximate best response, demonstrating generalization capability and improvement potential. We re-evaluate strong ZSC methods in the Overcooked environment using the proposed evaluation workflow. Surprisingly, the results in some of the most used layouts fail to distinguish the performance of different ZSC methods. Moreover, the evaluated ZSC methods must produce more diverse and high-performing training partners. Our proposed evaluation workflow calls for a change in how we efficiently evaluate ZSC methods as a supplement to human evaluation.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023quantifying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang*, Xihuai and Zhang*, Shao and Zhang, Wenhao and Dong, Wentao and Chen, Jingxiao and Wen, Ying and Zhang, Weinan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.05208}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Zero-shot Coordination</abbr></div>

        <!-- Entry bib key -->
        <div id="10.1613/jair.1.15884" class="col-sm-8">
        <!-- Title -->
        <div class="title">Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://liyang.page" rel="external nofollow noopener" target="_blank">Yang Li*</a>,¬†<em>Shao Zhang*</em>,¬†Jichen Sun,¬†Wenhao Zhang,¬†Yali Du,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†Xinbing Wang,¬†and¬†Wei Pan</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>J. Artif. Int. Res.</em>, Jul 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://dl.acm.org/doi/pdf/10.1613/jair.1.15884" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/liyang619/COLE-Platform" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Securing coordination between AI agent and teammates (human players or AI agents) in contexts involving unfamiliar humans continues to pose a significant challenge in Zero-Shot Coordination. The issue of cooperative incompatibility becomes particularly prominent when an AI agent is unsuccessful in synchronizing with certain previously unknown partners. Traditional algorithms have aimed to collaborate with partners by optimizing fixed objectives within a population, fostering diversity in strategies and behaviors. However, these techniques may lead to learning loss and an inability to cooperate with specific strategies within the population, a phenomenon named cooperative incompatibility in learning. In order to solve cooperative incompatibility in learning and effectively address the problem in the context of ZSC, we introduce the Cooperative Open-ended LEarning (COLE) framework, which formulates open-ended objectives in cooperative games with two players using perspectives of graph theory to evaluate and pinpoint the cooperative capacity of each strategy. We present two practical algorithms, specifically COLESV and COLER, which incorporate insights from game theory and graph theory. We also show that COLE could effectively overcome the cooperative incompatibility from theoretical and empirical analysis. Subsequently, we created an online Overcooked human-AI experiment platform, the COLE platform, which enables easy customization of questionnaires, model weights, and other aspects. Utilizing the COLE platform, we enlist 130 participants for human experiments. Our findings reveal a preference for our approach over state-of-the-art methods using a variety of subjective metrics. Moreover, objective experimental outcomes in the Overcooked game environment indicate that our method surpasses existing ones when coordinating with previously unencountered AI agents and the human proxy model. Our code and demo are publicly available at https://sites.google.com/view/cole-2023.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1613/jair.1.15884</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Yang and Zhang*, Shao and Sun, Jichen and Zhang, Wenhao and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{Sep 2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{AI Access Foundation}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{El Segundo, CA, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{80}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1076-9757}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1613/jair.1.15884}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1613/jair.1.15884}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{J. Artif. Int. Res.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{47}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AI-in-the-Loop</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang2023rethinking" class="col-sm-8">
        <!-- Title -->
        <div class="title">Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis</div>
        <!-- Author -->
        <div class="author">
        

        <em>Shao Zhang</em>,¬†Jianing Yu,¬†Xuhai Xu,¬†Changchang Yin,¬†<a href="https://yuxuan.lu/" rel="external nofollow noopener" target="_blank">Yuxuan Lu</a>,¬†Bingsheng Yao,¬†Melanie Tory,¬†Lace M. Padilla,¬†Jeffrey Caterino,¬†Ping Zhang,¬†and¬†Dakuo Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, May 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/abs/2309.12368.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Today‚Äôs AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023rethinking</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M. and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703300}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3613904.3642343}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3613904.3642343}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{445}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human-AI collaboration, Medical decision making, Sepsis diagnosis}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{&lt;conf-loc&gt;, &lt;city&gt;Honolulu&lt;/city&gt;, &lt;state&gt;HI&lt;/state&gt;, &lt;country&gt;USA&lt;/country&gt;, &lt;/conf-loc&gt;}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AI-in-the-Loop</abbr></div>

        <!-- Entry bib key -->
        <div id="yang2023talk2care" class="col-sm-8">
        <!-- Title -->
        <div class="title">Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://eugeniayang.github.io/" rel="external nofollow noopener" target="_blank">Ziqi Yang</a>,¬†Xuhai Xu,¬†Bingsheng Yao,¬†Ethan Rogers,¬†<em>Shao Zhang</em>,¬†Stephen Intille,¬†Nawar Shara,¬†Guodong Gordon Gao,¬†and¬†Dakuo Wang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>, May 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://dl.acm.org/doi/pdf/10.1145/3659625" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs‚Äô role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults‚Äô conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers‚Äô efforts and time. We envision our work as an initial exploration of LLMs‚Äô capability in the intersection of healthcare and interpersonal communication.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2023talk2care</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Ziqi and Xu, Xuhai and Yao, Bingsheng and Rogers, Ethan and Zhang, Shao and Intille, Stephen and Shara, Nawar and Gao, Guodong Gordon and Wang, Dakuo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{May 2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3659625}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3659625}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{73}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Large-language-model, Older adults, Patient-provider communication}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Zero-shot Coordination</abbr></div>

        <!-- Entry bib key -->
        <div id="COLE" class="col-sm-8">
        <!-- Title -->
        <div class="title">Cooperative Open-ended Learning Framework for Zero-Shot Coordination</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://liyang.page" rel="external nofollow noopener" target="_blank">Yang Li*</a>,¬†<em>Shao Zhang*</em>,¬†Jichen Sun,¬†Yali Du,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†Xinbing Wang,¬†and¬†Wei Pan</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Proceedings of the 40th International Conference on Machine Learning(ICML2023)</em>, May 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2302.04831.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/liyang619/COLE-Platform" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behavior diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overcome cooperative incompatibility. The experimental results in the Overcooked game environment demonstrate that our method outperforms current state-of-the-art methods when coordinating with different-level partners. Our code and demo are available at https://sites.google.com/view/cole-2023.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">COLE</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Yang and Zhang*, Shao and Sun, Jichen and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cooperative Open-ended Learning Framework for Zero-Shot Coordination}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning(ICML2023)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{PMLR 202}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Cooperative MARL, Zero-shot coordination, Open-ended learning}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AI-in-the-Loop</abbr></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.1002/gdj3.186" class="col-sm-8">
        <!-- Title -->
        <div class="title">GeoDeepShovel: A platform for building scientific database from geoscience literature with AI assistance</div>
        <!-- Author -->
        <div class="author">
        

        <em>Shao Zhang</em>,¬†<a href="https://xu-hui.netlify.app/" rel="external nofollow noopener" target="_blank">Hui Xu</a>,¬†<a href="https://www.ytjia.xyz/" rel="external nofollow noopener" target="_blank">Yuting Jia</a>,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†Dakuo Wang,¬†Luoyi Fu,¬†Xinbing Wang,¬†and¬†Chenghu Zhou</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Geoscience Data Journal</em>, May 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/Geoscience%20Data%20Journal%20-%202023%20-%20Zhang%20-%20GeoDeepShovel%20%20A%20platform%20for%20building%20scientific%20database%20from%20geoscience.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rapid development of big data science, the research paradigm in the field of geosciences has also begun to shift to big data-driven scientific discovery. Researchers need to read a huge amount of literature to locate, extract and aggregate relevant results and data that are published and stored in PDF format for building a scientific database to support the big data-driven discovery. In this paper, based on the findings of a study about how geoscientists annotate literature and extract and aggregate data, we proposed GeoDeepShovel, a publicly available AI-assisted data extraction system to support their needs. GeoDeepShovel leverages state-of-the-art neural network models to support researcher(s) easily and accurately annotate papers (in the PDF format) and extract data from tables, figures, maps, etc., in a human‚ÄìAI collaboration manner. As a part of the Deep-Time Digital Earth (DDE) program, GeoDeepShovel has been deployed for 8¬†months, and there are already 400 users from 44 geoscience research teams within the DDE program using it to construct scientific databases on a daily basis, and more than 240 projects and 50,000 documents have been processed for building scientific databases.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">https://doi.org/10.1002/gdj3.186</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Shao and Xu, Hui and Jia, Yuting and Wen, Ying and Wang, Dakuo and Fu, Luoyi and Wang, Xinbing and Zhou, Chenghu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GeoDeepShovel: A platform for building scientific database from geoscience literature with AI assistance}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Geoscience Data Journal}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{artificial intelligence, big data-driven discovery, data extraction, scientific database, human-computer interaction}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1002/gdj3.186}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/gdj3.186}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
</div>


          <!-- Social -->
        </article>

</div>

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5mkih6fvq9l&amp;m=0c&amp;c=ffffff&amp;cr1=ff9a9a&amp;f=comic_sans_ms&amp;l=1&amp;z=13&amp;hi=10&amp;cw=ffc3c3&amp;cb=ffffff" async="async"></script>
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Shao  ZHANG. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
