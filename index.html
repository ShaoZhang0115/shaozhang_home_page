<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Shao  ZHANG</title>
    <meta name="author" content="Shao  ZHANG">
    <meta name="description" content="">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%B0&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://shaozhang.info/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">Bio<span class="sr-only">(current)</span></a>
              </li>
              

              
              <!-- CV -->
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/CV_Shao_Zhang.pdf" target="_blank" rel="noopener noreferrer">cv</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
</li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
</li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Shao</span>  ZHANG
          </h1>
          <p class="desc">Ph.D. Candidate, <a href="https://www.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank"> Shanghai Jiao Tong University</a>, Shanghai, China.</p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/photo-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/photo-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/photo-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/photo.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="photo.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<!--
            <div class="address">
              <p>Shanghai Jiao Tong University</p> <p>Shanghai, China</p>

            </div> -->
            
            <div class="email">
              Email: <a href="mailto:shaozhang@sjtu.edu.cn">shaozhang@sjtu.edu.cn</a>
              <!-- <a href="mailto:zhang.shao.1@northeastern.edu">zhang.shao.1@northeastern.edu</a> -->
            </div>
            
          </div>

          <div class="clearfix">
            <p>Shao ZHANG is currently a Ph.D. candidate at Shanghai Jiao Tong University, supervised by <a href="https://yingwen.io/" rel="external nofollow noopener" target="_blank">Prof. Ying WEN</a> and <a href="https://www.cs.sjtu.edu.cn/~wang-xb/" rel="external nofollow noopener" target="_blank">Prof. Xinbing WANG</a>. <strong>Now she is a visiting student at Northeastern University under the supervision of <a href="https://www.dakuowang.com/" rel="external nofollow noopener" target="_blank">Prof. Dakuo WANG</a>.</strong></p>

<p>Shao obtained her MSc in Multimedia and Entertainment Technology (Game Development stream) from <a href="https://www.sd.polyu.edu.hk/en/" rel="external nofollow noopener" target="_blank">School of Design, The Hong Kong Polytechnic University</a> in 2020. Before that, Shao earned her B.Eng. in Industrial Design from <a href="http://design.hnu.edu.cn/" rel="external nofollow noopener" target="_blank">School of Design, Hunan University</a> in 2019. During her undergraduate study, she was a member of <a href="http://xiaolab.net/" rel="external nofollow noopener" target="_blank">XiaoLab</a> and was supervised by <a href="http://xiaolab.net/" rel="external nofollow noopener" target="_blank">Prof. Sheng XIAO</a> and <a href="http://ylsy.hnu.edu.cn/info/1353/9629.htm" rel="external nofollow noopener" target="_blank">Prof. Beibei ZHAN</a>.</p>

<p>Shao‚Äôs research interests include <strong>Human-AI Collaboration</strong> and <strong>Multi-agent System</strong>. 
Specifically, she is now focusing on:</p>
<ul>
  <li>
<strong>Human-in-the-Loop</strong> data annotation and <strong>AI-in-the-Loop</strong> deployment in real world scenario,</li>
  <li>Involving Large Language Models in Human-AI Collaboration,</li>
  <li>
<strong>Zero-shot Coordination</strong> in Human-AI Collaboration,</li>
  <li>
<strong>Bidirectional Alignment</strong> in Human-AI Team.</li>
</ul>

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>          
          <div class="news">
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row">Jan 19, 2024</th>
                  <td>
                    Our Paper <a href="https://arxiv.org/pdf/2309.12368.pdf" rel="external nofollow noopener" target="_blank">Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis</a> is accepted by CHI‚Äô24! See you in Hawaii!üèñÔ∏è
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Oct 20, 2023</th>
                  <td>
                    I will give an invited talk about our ICML2023 paper <a href="https://proceedings.mlr.press/v202/li23au.html" rel="external nofollow noopener" target="_blank">‚ÄúCooperative Open-ended Learning Framework for Zero-shot Coordination‚Äù</a> at <a href="http://www.adai.ai/dai/2023/index.html" rel="external nofollow noopener" target="_blank">DAI2023</a>. See you in Singapore on November 2023!
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Aug 20, 2023</th>
                  <td>
                    I‚Äôm excited to begin my visit to <a href="https://www.northeastern.edu/" rel="external nofollow noopener" target="_blank">Northeastern University</a> at Boston Campus! As of September 2023, I‚Äôll be a visiting PhD student at the NEU-HAI Lab, under the supervision of <a href="https://www.dakuowang.com/" rel="external nofollow noopener" target="_blank">Prof. Dakuo Wang</a>.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Aug 15, 2023</th>
                  <td>
                    A white paper about large-scale pre-trained and multi-agent reinforcement learning published in CCCF: <a href="https://dl.ccf.org.cn/article/articleDetail.html?type=xhtx_thesis&amp;_ack=1&amp;id=6605918436214784#10006-weixin-1-52626-6b3bffd01fdde4900130bc5a2751b6d1" rel="external nofollow noopener" target="_blank">Multi-agent Reinforcement Learning in (Open-ended) Multimodal Environments (in Chinese)</a>.

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Apr 25, 2023</th>
                  <td>
                    Our Paper <a href="https://openreview.net/forum?id=iMVMxYab0z" rel="external nofollow noopener" target="_blank">Cooperative Open-ended Learning Framework for Zero-Shot Coordination</a> is accepted in ICML 2023! Can‚Äôt wait to see you in Hawaii on July! üèñÔ∏è
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Apr 17, 2023</th>
                  <td>
                    I will serve as a student volunteer at <a href="https://chi2023.acm.org/" rel="external nofollow noopener" target="_blank">CHI2023</a>. See you in Hamburg on 23rd April!üòä
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Mar 15, 2023</th>
                  <td>
                    Our <a href="https://deepshovel.acemap.cn/#/" rel="external nofollow noopener" target="_blank">GeoDeepShovel‚Äôs</a> Table Extraction System for scientific literature is open-source now at <a href="https://github.com/ShaoZhang0115/Table-Extraction-for-Geoscience-Literature" rel="external nofollow noopener" target="_blank">GitHub</a> and <strong>the system demo is also avaliable at <a href="https://ddescholar.acemap.info/table-extraction" rel="external nofollow noopener" target="_blank">https://ddescholar.acemap.info/table-extraction</a></strong> !
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jan 7, 2023</th>
                  <td>
                    Our paper ‚Äú<a href="https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/gdj3.186" rel="external nofollow noopener" target="_blank">GeoDeepShovel: A Platform for Building Scientific Database from Geoscience Literature with AI Assistance</a>‚Äù gets accepted in Geoscience Data Journal. <strong>Our system is avaliable at <a href="https://deepshovel.acemap.cn" rel="external nofollow noopener" target="_blank">https://deepshovel.acemap.cn</a>.</strong>

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">May 22, 2022</th>
                  <td>
                    <a class="news-title" href="/news/announcement_1/">Announcement_1</a> 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>


          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ZSC</abbr></div>

        <!-- Entry bib key -->
        <div id="wang2023quantifying" class="col-sm-8">
        <!-- Title -->
        <div class="title">Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://xihuai18.github.io/" rel="external nofollow noopener" target="_blank">Xihuai Wang</a>,¬†<em>Shao Zhang</em>,¬†Wenhao Zhang,¬†Wentao Dong,¬†Jingxiao Chen,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†and¬†Weinan Zhang</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Preprint Under Review</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2310.05208.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2310.05208"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Zero-shot coordination (ZSC) is a new challenge focusing on generalizing learned coordination skills to unseen partners. Existing methods train the ego agent with partners from pre-trained or evolving populations. The agent‚Äôs ZSC capability is typically evaluated with a few evaluation partners, including human and agent, and reported by mean returns. Current evaluation methods for ZSC capability still need to improve in constructing diverse evaluation partners and comprehensively measuring the ZSC capability. We aim to create a reliable, comprehensive, and efficient evaluation method for ZSC capability. We formally define the ideal ‚Äôdiversity-complete‚Äô evaluation partners and propose the best response (BR) diversity, which is the population diversity of the BRs to the partners, to approximate the ideal evaluation partners. We propose an evaluation workflow including ‚Äôdiversity-complete‚Äô evaluation partners construction and a multi-dimensional metric, the Best Response Proximity (BR-Prox) metric. BR-Prox quantifies the ZSC capability as the performance similarity to each evaluation partner‚Äôs approximate best response, demonstrating generalization capability and improvement potential. We re-evaluate strong ZSC methods in the Overcooked environment using the proposed evaluation workflow. Surprisingly, the results in some of the most used layouts fail to distinguish the performance of different ZSC methods. Moreover, the evaluated ZSC methods must produce more diverse and high-performing training partners. Our proposed evaluation workflow calls for a change in how we efficiently evaluate ZSC methods as a supplement to human evaluation.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023quantifying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Xihuai and Zhang, Shao and Zhang, Wenhao and Dong, Wentao and Chen, Jingxiao and Wen, Ying and Zhang, Weinan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.05208}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint Under Review}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ZSC</abbr></div>

        <!-- Entry bib key -->
        <div id="Human-AI-COLE" class="col-sm-8">
        <!-- Title -->
        <div class="title">Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://liyang.page" rel="external nofollow noopener" target="_blank">Yang Li*</a>,¬†<em>Shao Zhang*</em>,¬†Jichen Sun,¬†Wenhao Zhang,¬†Yali Du,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†Xinbing Wang,¬†and¬†Wei Pan</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Preprint Under Review</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2306.03034.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="n/a"></span>
              <span class="__dimensions_badge_embed__" data-doi="n/a" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Achieving coordination between humans and artificial intelligence in scenarios involving previously unencountered humans remains a substantial obstacle within Zero-Shot Human-AI Coordination, which aims to develop AI agents capable of efficiently working alongside previously unknown human teammates. Traditional algorithms have aimed to collaborate with humans by optimizing fixed objectives within a population, fostering diversity in strategies and behaviors. However, these techniques may lead to learning loss and an inability to cooperate with specific strategies within the population, a phenomenon named cooperative incompatibility. To mitigate this issue, we introduce the Cooperative Open-ended LEarning (COLE) framework, which formulates open-ended objectives in cooperative games with two players using perspectives of graph theory to evaluate and pinpoint the cooperative capacity of each strategy. We put forth a practical algorithm incorporating insights from game theory and graph theory, e.g., Shapley Value and Centrality. We also show that COLE could effectively overcome the cooperative incompatibility from theoretical and empirical analysis. Subsequently, we created an online Overcooked human-AI experiment platform, the COLE platform, which enables easy customization of questionnaires, model weights, and other aspects. Utilizing the COLE platform, we enlist 130 participants for human experiments. Our findings reveal a preference for our approach over state-of-the-art methods using a variety of subjective metrics. Moreover, objective experimental outcomes in the Overcooked game environment indicate that our method surpasses existing ones when coordinating with previously unencountered AI agents and the human proxy model. </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Human-AI-COLE</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Yang and Zhang*, Shao and Sun, Jichen and Zhang, Wenhao and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprint Under Review}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Cooperative MARL, Zero-shot coordination, Open-ended learning}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ZSC</abbr></div>

        <!-- Entry bib key -->
        <div id="COLE" class="col-sm-8">
        <!-- Title -->
        <div class="title">Cooperative Open-ended Learning Framework for Zero-Shot Coordination</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://liyang.page" rel="external nofollow noopener" target="_blank">Yang Li*</a>,¬†<em>Shao Zhang*</em>,¬†Jichen Sun,¬†Yali Du,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†Xinbing Wang,¬†and¬†Wei Pan</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Proceedings of the 40th International Conference on Machine Learning(ICML2023)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2302.04831.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="n/a"></span>
              <span class="__dimensions_badge_embed__" data-doi="n/a" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behavior diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overcome cooperative incompatibility. The experimental results in the Overcooked game environment demonstrate that our method outperforms current state-of-the-art methods when coordinating with different-level partners. Our code and demo are available at https://sites.google.com/view/cole-2023.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">COLE</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Yang and Zhang*, Shao and Sun, Jichen and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cooperative Open-ended Learning Framework for Zero-Shot Coordination}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning(ICML2023)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{PMLR 202}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Cooperative MARL, Zero-shot coordination, Open-ended learning}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AI-in-the-Loop</abbr></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.1002/gdj3.186" class="col-sm-8">
        <!-- Title -->
        <div class="title">GeoDeepShovel: A platform for building scientific database from geoscience literature with AI assistance</div>
        <!-- Author -->
        <div class="author">
        

        <em>Shao Zhang</em>,¬†<a href="https://xu-hui.netlify.app/" rel="external nofollow noopener" target="_blank">Hui Xu</a>,¬†<a href="https://www.ytjia.xyz/" rel="external nofollow noopener" target="_blank">Yuting Jia</a>,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†<a href="https://dakuowang.com" rel="external nofollow noopener" target="_blank">Dakuo Wang</a>,¬†Luoyi Fu,¬†Xinbing Wang,¬†and¬†Chenghu Zhou</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Geoscience Data Journal</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/Geoscience%20Data%20Journal%20-%202023%20-%20Zhang%20-%20GeoDeepShovel%20%20A%20platform%20for%20building%20scientific%20database%20from%20geoscience.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/gdj3.186"></span>
              <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1002/gdj3.186" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rapid development of big data science, the research paradigm in the field of geosciences has also begun to shift to big data-driven scientific discovery. Researchers need to read a huge amount of literature to locate, extract and aggregate relevant results and data that are published and stored in PDF format for building a scientific database to support the big data-driven discovery. In this paper, based on the findings of a study about how geoscientists annotate literature and extract and aggregate data, we proposed GeoDeepShovel, a publicly available AI-assisted data extraction system to support their needs. GeoDeepShovel leverages state-of-the-art neural network models to support researcher(s) easily and accurately annotate papers (in the PDF format) and extract data from tables, figures, maps, etc., in a human‚ÄìAI collaboration manner. As a part of the Deep-Time Digital Earth (DDE) program, GeoDeepShovel has been deployed for 8¬†months, and there are already 400 users from 44 geoscience research teams within the DDE program using it to construct scientific databases on a daily basis, and more than 240 projects and 50,000 documents have been processed for building scientific databases.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">https://doi.org/10.1002/gdj3.186</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Shao and Xu, Hui and Jia, Yuting and Wen, Ying and Wang, Dakuo and Fu, Luoyi and Wang, Xinbing and Zhou, Chenghu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GeoDeepShovel: A platform for building scientific database from geoscience literature with AI assistance}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Geoscience Data Journal}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{n/a}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{artificial intelligence, big data-driven discovery, data extraction, scientific database, human-computer interaction}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1002/gdj3.186}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/gdj3.186}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AI-in-the-Loop</abbr></div>

        <!-- Entry bib key -->
        <div id="zhang2022knowledgeshovel" class="col-sm-8">
        <!-- Title -->
        <div class="title">KnowledgeShovel: An AI-in-the-Loop Document Annotation System for Scientific Knowledge Base Construction</div>
        <!-- Author -->
        <div class="author">
        

        <em>Shao Zhang</em>,¬†<a href="https://www.ytjia.xyz/" rel="external nofollow noopener" target="_blank">Yuting Jia</a>,¬†<a href="https://xu-hui.netlify.app/" rel="external nofollow noopener" target="_blank">Hui Xu</a>,¬†<a href="https://dakuowang.com" rel="external nofollow noopener" target="_blank">Dakuo Wang</a>,¬†<a href="https://toby.li" rel="external nofollow noopener" target="_blank">Toby Jia-Jun Li</a>,¬†<a href="https://yingwen.io" rel="external nofollow noopener" target="_blank">Ying Wen</a>,¬†Xinbing Wang,¬†and¬†Chenghu Zhou</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2210.02830</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2210.02830.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Constructing a comprehensive, accurate, and useful scientific knowledge base is crucial for human researchers synthesizing scientific knowledge and for enabling Al-driven scientific discovery. However, the current process is difficult, error-prone, and laborious due to (1) the enormous amount of scientific literature available; (2) the highly-specialized scientific domains; (3) the diverse modalities of information (text, figure, table); and, (4) the silos of scientific knowledge in different publications with inconsistent formats and structures. Informed by a formative study and iterated with participatory design workshops, we designed and developed KnowledgeShovel, an Al-in-the-Loop document annotation system for researchers to construct scientific knowledge bases. The design of KnowledgeShovel introduces a multi-step multi-modal human-AI collaboration pipeline that aligns with users‚Äô existing workflows to improve data accuracy while reducing the human burden. A follow-up user evaluation with 7 geoscience researchers shows that KnowledgeShovel can enable efficient construction of scientific knowledge bases with satisfactory accuracy.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2022knowledgeshovel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{KnowledgeShovel: An AI-in-the-Loop Document Annotation System for Scientific Knowledge Base Construction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Shao and Jia, Yuting and Xu, Hui and Wang, Dakuo and Li, Toby Jia-Jun and Wen, Ying and Wang, Xinbing and Zhou, Chenghu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2210.02830}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                            <!--<a href="mailto:%73%68%61%6F%7A%68%61%6E%67@%73%6A%74%75.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
             --><a href="https://orcid.org/0000-0002-0111-0776" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=UG36L2YAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.semanticscholar.org/author/2116577679" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a>
            <a href="https://github.com/ShaoZhang0115#%20your%20GitHub%20user%20name" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            

              </div>

              <!-- <div class="contact-note">
                Contact me by shaozhang@sjtu.edu.cn

              </div> -->
              
            </div>
        </article>

</div>

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5mkih6fvq9l&amp;m=0c&amp;c=ffffff&amp;cr1=ff9a9a&amp;f=comic_sans_ms&amp;l=1&amp;z=13&amp;hi=10&amp;cw=ffc3c3&amp;cb=ffffff" async="async"></script>
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2024 Shao  ZHANG. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
